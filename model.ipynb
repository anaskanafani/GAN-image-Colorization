{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import models, layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 178\n",
    "IMG_HEIGHT = 218\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.utils.vis_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.vis_utils'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# The batch size we'll use for training\n",
    "batch_size = 64\n",
    "\n",
    "# These many images will be used from the data archive\n",
    "dataset_split = 2500\n",
    "\n",
    "master_dir = 'data'\n",
    "x = []\n",
    "y = []\n",
    "for image_file in os.listdir( master_dir )[ 0 : dataset_split ]:\n",
    "    rgb_image = Image.open( os.path.join( master_dir , image_file ) ).resize( ( IMG_HEIGHT , IMG_WIDTH ) )\n",
    "    # Normalize the RGB image array\n",
    "    rgb_img_array = (np.asarray( rgb_image ) ) / 255\n",
    "    gray_image = rgb_image.convert( 'L' )\n",
    "    # Normalize the grayscale image array\n",
    "    gray_img_array = ( np.asarray( gray_image ).reshape( ( IMG_HEIGHT , IMG_WIDTH , 1 ) ) ) / 255\n",
    "    # Append both the image arrays\n",
    "    x.append( gray_img_array )\n",
    "    y.append( rgb_img_array )\n",
    "\n",
    "# Train-test splitting\n",
    "train_x, test_x, train_y, test_y = train_test_split( np.array(x) , np.array(y) , test_size=0.1 )\n",
    "\n",
    "# Construct tf.data.Dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices( ( train_x , train_y ) )\n",
    "dataset = dataset.batch( batch_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/img_align_celeba'\n",
    "image_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir)]\n",
    "\n",
    "def convert_to_rgb(image):\n",
    "    # Assuming image is grayscale with shape [IMG_HEIGHT, IMG_WIDTH, 1]\n",
    "    return tf.image.grayscale_to_rgb(image)  # Converts grayscale image to RGB\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    image = (image / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "    return image\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.cond(\n",
    "        tf.strings.regex_full_match(image_path, '.*\\\\.jpg$'),\n",
    "        lambda: tf.image.decode_jpeg(image, channels=3),\n",
    "        lambda: tf.image.decode_png(image, channels=3)\n",
    "    )\n",
    "    image = preprocess_image(image)\n",
    "    return image\n",
    "\n",
    "# Dataset preparation\n",
    "dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "dataset = dataset.map(lambda x: load_image(x))\n",
    "dataset = dataset.batch(BATCH_SIZE).shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/img_align_celeba'\n",
    "image_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir)]\n",
    "\n",
    "def convert_to_rgb(image):\n",
    "    # Assuming image is grayscale with shape [IMG_HEIGHT, IMG_WIDTH, 1]\n",
    "    return tf.image.grayscale_to_rgb(image)  # Converts grayscale image to RGB\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    image = (image / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "    return image\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.cond(\n",
    "        tf.strings.regex_full_match(image_path, '.*\\\\.jpg$'),\n",
    "        lambda: tf.image.decode_jpeg(image, channels=3),\n",
    "        lambda: tf.image.decode_png(image, channels=3)\n",
    "    )\n",
    "    image = preprocess_image(image)\n",
    "    return image\n",
    "\n",
    "# Dataset preparation\n",
    "dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "dataset = dataset.map(lambda x: load_image(x))\n",
    "dataset = dataset.batch(BATCH_SIZE).shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, train_size=0.8):\n",
    "    # dataset = dataset.shuffle(buffer_size=10000, seed=42)\n",
    "    \n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(dataset_size * train_size)\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    test_dataset = dataset.skip(train_size)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset: {len(train_dataset)} batches\")\n",
    "print(f\"Test dataset: {len(test_dataset)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize and Normalize the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda x: tf.image.resize(x, (IMG_HEIGHT, IMG_WIDTH)))\n",
    "train_dataset = train_dataset.map(lambda x: x / 255.0)\n",
    "train_dataset = train_dataset.map(lambda x: tf.image.rgb_to_grayscale(x))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().reshape(IMG_HEIGHT, IMG_WIDTH), cmap='gray')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the datasets for performance\n",
    "\n",
    "Cache, Shuffle, and Prefetch the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# # test_dataset = test_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# # remove labels\n",
    "# ds = ds.map(lambda x: x[\"image\"])\n",
    "# # scale images\n",
    "# ds = ds.map(lambda x: scale(x))\n",
    "# ds = ds.cache()\n",
    "# ds = ds.shuffle(1000)\n",
    "# ds = ds.batch(128)\n",
    "# ds = ds.prefetch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers for encoding\n",
    "    model.add(layers.Conv2D(32, kernel_size=4, strides=1, padding='same', input_shape=[IMG_HEIGHT, IMG_WIDTH, 1]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, kernel_size=4, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, kernel_size=4, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(32, kernel_size=4, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(3, kernel_size=4, strides=1, padding='same', activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = generator.predict(images)\n",
    "plt.imshow(img[0, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers for encoding\n",
    "    model.add(layers.Conv2D(32, kernel_size=4, strides=2, padding='same', input_shape=[IMG_HEIGHT, IMG_WIDTH, 3]))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(64, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(layers.Conv2D(256, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(img)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_optimizer = tf.keras.optimizers.legacy.Adam(1e-4)\n",
    "# discriminator_optimizer = tf.keras.optimizers.legacy.Adam(1e-4)\n",
    "generator_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "save_interval = 2\n",
    "# You will reuse this seed overtime\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step function\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    # Generate images using the generator\n",
    "    noise = tf.random.normal([BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, 1])  # Input noise or grayscale images\n",
    "    generated_images = generator(noise, training=True)\n",
    "\n",
    "    # Get the discriminator's output for real and fake images\n",
    "    real_output = discriminator(real_images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    # Calculate the losses\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Apply gradients to optimize the generator\n",
    "    gradients_of_generator = tf.GradientTape().gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    # Apply gradients to optimize the discriminator\n",
    "    gradients_of_discriminator = tf.GradientTape().gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def generate_and_save_images(generator, epoch, test_input, save_path='generated_images'):\n",
    "    \"\"\"\n",
    "    Generate and save images from the Generator model.\n",
    "    \n",
    "    Parameters:\n",
    "    - generator: The Generator model.\n",
    "    - epoch: The current epoch number (for saving image with epoch information).\n",
    "    - test_input: Random noise or grayscale images to generate images.\n",
    "    - save_path: Directory path to save the generated images.\n",
    "    \"\"\"\n",
    "    # Generate images\n",
    "    predictions = generator(test_input, training=False)\n",
    "    \n",
    "    # Normalize images to the range [0, 1] for visualization\n",
    "    predictions = (predictions + 1) / 2.0\n",
    "\n",
    "    # Create a grid of images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(predictions[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Save the generated images\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    plt.savefig(f'{save_path}/image_at_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Example usage:\n",
    "# Generate and save images after each epoch\n",
    "# generate_and_save_images(generator, epoch, test_input, save_path='generated_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(image_batch)\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            # Generate and save images\n",
    "            test_input = tf.random.normal([16, IMG_HEIGHT, IMG_WIDTH, 1])  # Generate some test images\n",
    "            generate_and_save_images(generator, epoch + 1, test_input)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Gen Loss: {gen_loss.numpy()}, Disc Loss: {disc_loss.numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch in train_dataset.take(1):\n",
    "    print(image_batch.shape)  # Should match [batch_size, IMG_HEIGHT, IMG_WIDTH, 3]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(image_batch.shape[0]):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow((image_batch[i] + 1) / 2.0)  # Assuming images are normalized to [-1, 1]\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
