# **üé® Generative Adversarial Network (GAN) for Image Colorization**

## üìñ About
This project implements a Generative Adversarial Network (GAN) for the task of image colorization. The architecture consists of two main components‚Äîa **Generator** that tries to generate  colored images from gray-scale inputs, and a **Discriminator** which is supposed to discriminate between real color images and those generated by the Generator. The model is trained to enhance the quality of generated images over time, gradually producing realistic colorized images from grayscale inputs.

## üåç Impact
This GAN-based approach to image colorization can be a powerful tool for various applications including:
- Restoring and colorizing old black-and-white photos.
- Assisting in data augmentation for machine learning tasks.

## üß† Methodology

### What is a GAN?
A **Generative Adversarial Network (GAN)** is a class of machine learning models consisting of two neural networks: a **Generator** and a **Discriminator**. These two networks are trained together through an adversarial process:
- The **Generator** creates images from random or specific input data.
- The **Discriminator** evaluates the authenticity of images (whether they are real or generated).

### Generator
The **Generator** network in this project is a convolutional neural network that takes a grayscale image as input and outputs a colorized version of the image. The model architecture is designed with multiple convolutional layers followed by transposed convolutional layers for upsampling.

- **Input:** Grayscale image of shape `(150, 150, 1)`.
- **Output:** Colorized image of shape `(150, 150, 3)`.

#### Layer-by-Layer Explanation
1. **Input Layer**: Accepts the grayscale image input.
2. **Conv2D Layers**: Multiple convolutional layers with Leaky ReLU activation functions are used to extract features from the grayscale image.
3. **Bottleneck Layer**: A dense layer that condenses the information before upsampling.
4. **Conv2DTranspose Layers**: Transposed convolutional layers are used to upsample the image back to its original resolution but with 3 channels, representing the colorized image.

### Discriminator
The **Discriminator** network is another convolutional neural network that attempts to distinguish between real and generated (fake) images. It outputs a probability indicating the likelihood that an input image is real.

- **Input:** Color image of shape `(150, 150, 3)`.
- **Output:** Scalar probability indicating real (1) or fake (0).

#### Layer-by-Layer Explanation
1. **Input Layer**: Accepts the color image input.
2. **Conv2D Layers**: Convolutional layers with ReLU activations are used to downsample the image and extract features.
3. **MaxPooling Layers**: Applied after some convolutional layers to reduce the spatial dimensions.
4. **Dense Layers**: Fully connected layers leading to a final dense layer with a sigmoid activation to output the probability.

## üíª Technologies Used
- **TensorFlow** and **Keras** for model building and training.
- **NumPy** for data manipulation.
- **Matplotlib** for visualizations.
- **PIL (Python Imaging Library)** for image processing.

## üîç Code Snippets

### Loading and Preparing Data
```python
x, y = [], []
for image_file in os.listdir(DATA_DIR)[:DATA_SPLIT]:
    rgb_image = Image.open(os.path.join(DATA_DIR, image_file)).resize((IMG_WIDTH, IMG_HEIGHT))
    rgb_img_array = np.asarray(rgb_image) / 255
    gray_image = rgb_image.convert('L')
    gray_img_array = np.asarray(gray_image).reshape((IMG_WIDTH, IMG_HEIGHT, 1)) / 255
    x.append(gray_img_array)
    y.append(rgb_img_array)

train_x, test_x, train_y, test_y = train_test_split(np.array(x), np.array(y), test_size=0.1)
```

### Defining the Generator Model
```python
def make_generator():

    inputs = tf.keras.layers.Input( shape=( IMG_HEIGHT , IMG_HEIGHT , 1 ) )

    conv1 = tf.keras.layers.Conv2D( 16 , kernel_size=( 5 , 5 ) , strides=1 )( inputs )
    conv1 = tf.keras.layers.LeakyReLU()( conv1 )
    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )
    conv1 = tf.keras.layers.LeakyReLU()( conv1 )
    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )
    conv1 = tf.keras.layers.LeakyReLU()( conv1 )

    conv2 = tf.keras.layers.Conv2D( 32 , kernel_size=( 5 , 5 ) , strides=1)( conv1 )
    conv2 = tf.keras.layers.LeakyReLU()( conv2 )
    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )
    conv2 = tf.keras.layers.LeakyReLU()( conv2 )
    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )
    conv2 = tf.keras.layers.LeakyReLU()( conv2 )

    conv3 = tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1 )( conv2 )
    conv3 = tf.keras.layers.LeakyReLU()( conv3 )
    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )
    conv3 = tf.keras.layers.LeakyReLU()( conv3 )
    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )
    conv3 = tf.keras.layers.LeakyReLU()( conv3 )

    bottleneck = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='tanh' , padding='same' )( conv3 )

    concat_1 = tf.keras.layers.Concatenate()( [ bottleneck , conv3 ] )
    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_1 )
    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_3 )
    conv_up_3 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_3 )

    concat_2 = tf.keras.layers.Concatenate()( [ conv_up_3 , conv2 ] )
    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_2 )
    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_2 )
    conv_up_2 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_2 )

    concat_3 = tf.keras.layers.Concatenate()( [ conv_up_2 , conv1 ] )
    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( concat_3 )
    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( conv_up_1 )
    conv_up_1 = tf.keras.layers.Conv2DTranspose( 3 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu')( conv_up_1 )

    model = tf.keras.models.Model( inputs , conv_up_1 )
    return model
```

### Defining the Discriminator Model
```python
def make_discriminator():
    layers = [
        tf.keras.layers.Conv2D( 32 , kernel_size=( 7 , 7 ) , strides=1 , activation='relu' , input_shape=(IMG_HEIGHT, IMG_WIDTH , 3 ) ),
        tf.keras.layers.Conv2D( 32 , kernel_size=( 7, 7 ) , strides=1, activation='relu'  ),
        tf.keras.layers.MaxPooling2D(),
        
        tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1, activation='relu'  ),
        tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1, activation='relu'  ),
        tf.keras.layers.MaxPooling2D(),
        
        tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),
        tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),
        tf.keras.layers.MaxPooling2D(),
        
        tf.keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),
        tf.keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),
        tf.keras.layers.MaxPooling2D(),
        
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense( 512, activation='relu'  )  ,
        tf.keras.layers.Dense( 128 , activation='relu' ) ,
        tf.keras.layers.Dense( 16 , activation='relu' ) ,
        tf.keras.layers.Dense( 1 , activation='sigmoid' )
    ]
    model = tf.keras.models.Sequential( layers )
    return model
```

## üìä Visualizations and Findings

### Dataset Samples

![b5dc71f0-4f1d-40fc-90af-d5df7fd54d7f](https://github.com/user-attachments/assets/1b0a59af-f260-4970-bd9b-80ee17ab3f40)

This image shows a preview of some data samples in the dataset used for training. It includes grayscale input images that the GAN will learn to colorize.

### Generator and Discriminator Loss Over Time

![2f431e93-f1ac-46b8-85b3-0a26fb4988b9](https://github.com/user-attachments/assets/d0c0c003-30d9-4775-a229-12d5839448a6)

The plot below shows the loss curves for both the Generator and Discriminator during the training process. Monitoring these losses helps in understanding the training dynamics and ensuring the GAN is learning effectively.

### Generated Colorized Images After 150 Epochs

![71ceb5cd-a485-4a78-84ec-23378f032ef4](https://github.com/user-attachments/assets/b57d0188-be17-4f9d-bac7-3845e1bf2d69)

The following images display the generated colorized outputs after training the GAN for 150 epochs. Although these results show some promise, they are not yet optimal. More epochs would likely improve the quality and realism of the generated images.


## üöß Challenges and How I Overcame Them
- **Challenge:** Balancing the learning rates for the Generator and Discriminator to prevent one from overpowering the other.
  - **Solution:** Used separate Adam optimizers with fine-tuned learning rates for both networks.
  
- **Challenge:** Ensuring stability during training, as GANs are notoriously difficult to train.
  - **Solution:** Implemented gradient clipping and regular checkpoints to restore stable models.

## üîö Conclusion
In this project, I have demonstrated the viability of using GANs for the task of image colorization. The model can generate colored images from grayscale input, showing the effectiveness of the GAN architecture in this task. 
